diff --git a/.github/ISSUE_TEMPLATE/bug_report.md b/.github/ISSUE_TEMPLATE/bug_report.md
index d43f812..d50efb3 100644
--- a/.github/ISSUE_TEMPLATE/bug_report.md
+++ b/.github/ISSUE_TEMPLATE/bug_report.md
@@ -3,7 +3,7 @@ name: Bug report
 about: Create a detailed report on the bug and it's root cause. Conduct root cause error analysis
 title: "[BUG] "
 labels: bug
-assignees: kyegomez
+assignees: zeus
 
 ---
 
diff --git a/.github/ISSUE_TEMPLATE/feature_request.md b/.github/ISSUE_TEMPLATE/feature_request.md
index 806abd7..4d09d48 100644
--- a/.github/ISSUE_TEMPLATE/feature_request.md
+++ b/.github/ISSUE_TEMPLATE/feature_request.md
@@ -3,7 +3,7 @@ name: Feature request
 about: Suggest an idea for this project
 title: ''
 labels: ''
-assignees: 'kyegomez'
+assignees: 'zeus'
 
 ---
 
diff --git a/.github/PULL_REQUEST_TEMPLATE.yml b/.github/PULL_REQUEST_TEMPLATE.yml
index 8e03012..c3a453d 100644
--- a/.github/PULL_REQUEST_TEMPLATE.yml
+++ b/.github/PULL_REQUEST_TEMPLATE.yml
@@ -1,22 +1,3 @@
-<!-- Thank you for contributing to Zeta!
+<!-- Thank you for contributing to AlarioAI!
 
-Replace this comment with:
-  - Description: a description of the change, 
-  - Issue: the issue # it fixes (if applicable),
-  - Dependencies: any dependencies required for this change,
-  - Tag maintainer: for a quicker response, tag the relevant maintainer (see below),
-  - Twitter handle: we announce bigger features on Twitter. If your PR gets announced and you'd like a mention, we'll gladly shout you out!
-
-If you're adding a new integration, please include:
-  1. a test for the integration, preferably unit tests that do not rely on network access,
-  2. an example notebook showing its use.
-
-Maintainer responsibilities:
-  - nn / Misc / if you don't know who to tag: kye@apac.ai
-  - tokenizers: kye@apac.ai
-  - training / Prompts: kye@apac.ai
-  - models: kye@apac.ai
-
-If no one reviews your PR within a few days, feel free to kye@apac.ai
-
-See contribution guidelines for more information on how to write/run tests, lint, etc: https://github.com/kyegomez/zeta
\ No newline at end of file
+WIP
\ No newline at end of file
diff --git a/.github/dependabot.yml b/.github/dependabot.yml
deleted file mode 100644
index 34b75fb..0000000
--- a/.github/dependabot.yml
+++ /dev/null
@@ -1,14 +0,0 @@
-# https://docs.github.com/en/code-security/supply-chain-security/keeping-your-dependencies-updated-automatically/configuration-options-for-dependency-updates
-
-version: 2
-updates:
-  - package-ecosystem: "github-actions"
-    directory: "/"
-    schedule:
-      interval: "weekly"
-
-  - package-ecosystem: "pip"
-    directory: "/"
-    schedule:
-      interval: "weekly"
-
diff --git a/.github/workflows/code_quality_control.yml b/.github/workflows/code_quality_control.yml
deleted file mode 100644
index 4995f92..0000000
--- a/.github/workflows/code_quality_control.yml
+++ /dev/null
@@ -1,30 +0,0 @@
-name: Linting and Formatting
-
-on:
-  push:
-    branches:
-      - main
-
-jobs:
-  lint_and_format:
-    runs-on: ubuntu-latest
-
-    steps:
-      - name: Checkout code
-        uses: actions/checkout@v4
-
-      - name: Set up Python
-        uses: actions/setup-python@v5
-        with:
-          python-version: 3.x
-
-      - name: Install dependencies
-        run: pip install -r requirements.txt
-
-      - name: Find Python files
-        run: find swarms_torch -name "*.py" -type f -exec autopep8 --in-place --aggressive --aggressive {} +
-
-      - name: Push changes
-        uses: ad-m/github-push-action@master
-        with:
-          github_token: ${{ secrets.GITHUB_TOKEN }}
\ No newline at end of file
diff --git a/.github/workflows/cos_integration.yml b/.github/workflows/cos_integration.yml
deleted file mode 100644
index c826e62..0000000
--- a/.github/workflows/cos_integration.yml
+++ /dev/null
@@ -1,42 +0,0 @@
-name: Continuous Integration
-
-on:
-  push:
-    branches:
-      - main
-
-jobs:
-  test:
-    runs-on: ubuntu-latest
-    steps:
-      - name: Checkout code
-        uses: actions/checkout@v4
-
-      - name: Set up Python
-        uses: actions/setup-python@v5
-        with:
-          python-version: 3.x
-
-      - name: Install dependencies
-        run: pip install -r requirements.txt
-
-      - name: Run unit tests
-        run: pytest tests/unit
-
-      - name: Run integration tests
-        run: pytest tests/integration
-
-      - name: Run code coverage
-        run: pytest --cov=swarms tests/
-
-      - name: Run linters
-        run: pylint swarms
-
-      - name: Build documentation
-        run: make docs
-
-      - name: Validate documentation
-        run: sphinx-build -b linkcheck docs build/docs
-
-      - name: Run performance tests
-        run: pytest tests/performance
\ No newline at end of file
diff --git a/.github/workflows/docs.yml b/.github/workflows/docs.yml
deleted file mode 100644
index 7fb194d..0000000
--- a/.github/workflows/docs.yml
+++ /dev/null
@@ -1,19 +0,0 @@
-name: Docs WorkFlow
-
-on:
-  push:
-    branches:
-      - master
-      - main
-      - develop
-jobs:
-  deploy:
-    runs-on: ubuntu-latest
-    steps:
-      - uses: actions/checkout@v4
-      - uses: actions/setup-python@v5
-        with:
-          python-version: 3.x
-      - run: pip install mkdocs-material
-      - run: pip install "mkdocstrings[python]"
-      - run: mkdocs gh-deploy --force
\ No newline at end of file
diff --git a/.github/workflows/docs_test.yml b/.github/workflows/docs_test.yml
deleted file mode 100644
index 1820a0a..0000000
--- a/.github/workflows/docs_test.yml
+++ /dev/null
@@ -1,28 +0,0 @@
-name: Documentation Tests
-
-on:
-  push:
-    branches:
-      - master
-
-jobs:
-  test:
-    runs-on: ubuntu-latest
-
-    steps:
-      - name: Checkout code
-        uses: actions/checkout@v4
-
-      - name: Set up Python
-        uses: actions/setup-python@v5
-        with:
-          python-version: 3.x
-
-      - name: Install dependencies
-        run: pip install -r requirements.txt
-
-      - name: Build documentation
-        run: make docs
-
-      - name: Validate documentation
-        run: sphinx-build -b linkcheck docs build/docs
\ No newline at end of file
diff --git a/.github/workflows/label.yml b/.github/workflows/label.yml
deleted file mode 100644
index d23c4d4..0000000
--- a/.github/workflows/label.yml
+++ /dev/null
@@ -1,22 +0,0 @@
-# This workflow will triage pull requests and apply a label based on the
-# paths that are modified in the pull request.
-#
-# To use this workflow, you will need to set up a .github/labeler.yml
-# file with configuration.  For more information, see:
-# https://github.com/actions/labeler
-
-name: Labeler
-on: [pull_request_target]
-
-jobs:
-  label:
-
-    runs-on: ubuntu-latest
-    permissions:
-      contents: read
-      pull-requests: write
-
-    steps:
-    - uses: actions/labeler@v5
-      with:
-        repo-token: "${{ secrets.GITHUB_TOKEN }}"
diff --git a/.github/workflows/lints.yml b/.github/workflows/lints.yml
deleted file mode 100644
index 9dbb11f..0000000
--- a/.github/workflows/lints.yml
+++ /dev/null
@@ -1,25 +0,0 @@
-name: Linting
-
-on:
-  push:
-    branches:
-      - master
-
-jobs:
-  lint:
-    runs-on: ubuntu-latest
-
-    steps:
-      - name: Checkout code
-        uses: actions/checkout@v4
-
-      - name: Set up Python
-        uses: actions/setup-python@v5
-        with:
-          python-version: 3.x
-
-      - name: Install dependencies
-        run: pip install -r requirements.txt
-
-      - name: Run linters
-        run: pylint swarms_torch
\ No newline at end of file
diff --git a/.github/workflows/pr_request_checks.yml b/.github/workflows/pr_request_checks.yml
deleted file mode 100644
index b8078b1..0000000
--- a/.github/workflows/pr_request_checks.yml
+++ /dev/null
@@ -1,27 +0,0 @@
-name: Pull Request Checks
-
-on:
-  pull_request:
-    branches:
-      - master
-
-jobs:
-  test:
-    runs-on: ubuntu-latest
-
-    steps:
-      - name: Checkout code
-        uses: actions/checkout@v4
-
-      - name: Set up Python
-        uses: actions/setup-python@v5
-        with:
-          python-version: 3.x
-
-      - name: Install dependencies
-        run: pip install -r requirements.txt
-
-      - name: Run tests and checks
-        run: |
-          pytest tests/
-          pylint swarms_torch
\ No newline at end of file
diff --git a/.github/workflows/pull-request-links.yml b/.github/workflows/pull-request-links.yml
deleted file mode 100644
index e5812fb..0000000
--- a/.github/workflows/pull-request-links.yml
+++ /dev/null
@@ -1,18 +0,0 @@
-name: readthedocs/actions
-on:
-  pull_request_target:
-    types:
-      - opened
-    paths:
-      - "docs/**"
-
-permissions:
-  pull-requests: write
-
-jobs:
-  pull-request-links:
-    runs-on: ubuntu-latest
-    steps:
-      - uses: readthedocs/actions/preview@v1
-        with:
-          project-slug: swarms_torch
\ No newline at end of file
diff --git a/.github/workflows/python-publish.yml b/.github/workflows/python-publish.yml
deleted file mode 100644
index e231fcf..0000000
--- a/.github/workflows/python-publish.yml
+++ /dev/null
@@ -1,32 +0,0 @@
-
-name: Upload Python Package
-
-on:
-  release:
-    types: [published]
-
-permissions:
-  contents: read
-
-jobs:
-  deploy:
-
-    runs-on: ubuntu-latest
-
-    steps:
-    - uses: actions/checkout@v4
-    - name: Set up Python
-      uses: actions/setup-python@v5
-      with:
-        python-version: '3.x'
-    - name: Install dependencies
-      run: |
-        python -m pip install --upgrade pip
-        pip install build
-    - name: Build package
-      run: python -m build
-    - name: Publish package
-      uses: pypa/gh-action-pypi-publish@e53eb8b103ffcb59469888563dc324e3c8ba6f06
-      with:
-        user: __token__
-        password: ${{ secrets.PYPI_API_TOKEN }}
\ No newline at end of file
diff --git a/.github/workflows/pylint.yml b/.github/workflows/python-test.yml
similarity index 50%
rename from .github/workflows/pylint.yml
rename to .github/workflows/python-test.yml
index d3f42fb..9dfa886 100644
--- a/.github/workflows/pylint.yml
+++ b/.github/workflows/python-test.yml
@@ -1,23 +1,26 @@
-name: Pylint
-
-on: [push]
+name: Python Tests
 
+on:
+  pull_request:
+    branches: [main, integration]
+  
 jobs:
-  build:
+  tests:
     runs-on: ubuntu-latest
+
     strategy:
       matrix:
-        python-version: ["3.8", "3.9", "3.10"]
+        python-version: ["3.11"]
+    
     steps:
     - uses: actions/checkout@v4
     - name: Set up Python ${{ matrix.python-version }}
-      uses: actions/setup-python@v5
+      uses: actions/setup-python@4
       with:
         python-version: ${{ matrix.python-version }}
-    - name: Install dependencies
+    -name: Install dependencies
       run: |
         python -m pip install --upgrade pip
-        pip install pylint
-    - name: Analysing the code with pylint
-      run: |
-        pylint $(git ls-files '*.py')
+        pip install -e '.[testing]'
+    - name: Run tests with pytest
+      run: pytest tests
\ No newline at end of file
diff --git a/README.md b/README.md
index 5061c20..05a6f26 100644
--- a/README.md
+++ b/README.md
@@ -1,31 +1,32 @@
-[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)
-
 # BitNet
-![bitnet](/bitnet.png)
-PyTorch Implementation of the linear methods and model from the paper "BitNet: Scaling 1-bit Transformers for Large Language Models"
+![bitnet](/assets/main_image.png)
+PyTorch implementations for training and evaluating 1.58-bits neural networks. It covers methods and models from the following research papers:
 
-[Paper link:](https://arxiv.org/pdf/2310.11453.pdf)
 
-BitLinear = tensor -> layernorm -> Binarize -> abs max quantization -> dequant
+## Papers
+* [BitNet: Scaling 1-bit Transformers for Large Language Models:](https://arxiv.org/pdf/2310.11453.pdf)
+    "The implementation of the BitNet architecture is quite simple, requiring only the replacement of linear projections (i.e., nn.Linear in PyTorch) in the Transformer. " -- BitNet is really easy to implement just swap out the linears with the BitLinear modules! 
+* [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764.pdf)
+    **Extends BitNet to 1.58 bits:** Introduces modifications to the original BitNet architecture, including a novel quantization function (absmean) for the weights and a revised approach for handling activation outputs. These changes enable training with 1.58-bit weights while maintaining efficiency and performance.
 
-"The implementation of the BitNet architecture is quite simple, requiring only the replacement of linear projections (i.e., nn.Linear in PyTorch) in the Transformer. " -- BitNet is really easy to implement just swap out the linears with the BitLinear modules! 
 
-## **NEWS**
-- BitNet Transformer has been trained using the `train.py` file that trains on enwiki8 a small 1gb dataset of wikipedia: [HERE IS THE LINK](https://drive.google.com/file/d/1gBuZRFBqMV3cVD902LXA_hmZl4e0dLyY/view)
-- **New Iteration** 🔥 There is an all-new iteration from the paper "[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)", we're implementing it now. Join the Agora discord and contribute! [Join Here](https://discord.gg/hFzevCjG8c)
-- **New Optimizations** The first `BitLinear` has been optimized and we now have a Bit Attention `BitMGQA` That implements BitLinear into the attention mechanism. Multi Grouped Query Attention is also widely recognized as the best attention for its fast decoding and long context handling, thanks to Frank for his easy to use implementation!
+## Installation
 
-## Appreciation
-- Dimitry, Nullonix for analysis and code review and revision
-- Vyom, for providing 4080 to train!
+### From source
+```sh
+git clone git@github.com:AlarioAI/BitNet.git
+cd BitNet
+python3.11 -m pip install -e .
+```
 
-## Installation
-`pip install bitnet`
+### Direct (main)
+```sh
+python3.11 -m pip install https://github.com/AlarioAI/BitNet.git
+```
 
 ## Usage:
 
 ### `BitLinear`
-- Example of the BitLinear layer which is the main innovation of the paper!
 ```python
 import torch
 
@@ -44,136 +45,18 @@ print(y)
 ```
 ----
 
-### `BitNetTransformer`
-- Fully implemented Transformer as described in the diagram with MHA, and BitFeedforwards
-- Can be utilized not just for text but for images and maybe even video or audio processing
-- Complete with residuals and skip connections for gradient flow
-
-```python
-# Import the necessary libraries
-import torch
-from bitnet import BitNetTransformer
-
-# Create a random tensor of integers
-x = torch.randint(0, 20000, (1, 1024))
-
-# Initialize the BitNetTransformer model
-bitnet = BitNetTransformer(
-    num_tokens=20000,  # Number of unique tokens in the input
-    dim=1024,  # Dimension of the input and output embeddings
-    depth=6,  # Number of transformer layers
-    heads=8,  # Number of attention heads
-    ff_mult=4,  # Multiplier for the hidden dimension in the feed-forward network
-)
-
-# Pass the tensor through the transformer model
-logits = bitnet(x)
-
-# Print the shape of the output
-print(logits)
-
-```
-
-
-### `BitAttention`
-This Attention has been modified to use BitLinear instead of the default linear projection. It's also using Multi-Grouped Query Attention instead of regular multi-head attention for faster decoding and longer context handling.
-
-```python
-import torch
-from bitnet import BitMGQA
-
-# Create a random tensor of shape (1, 10, 512)
-x = torch.randn(1, 10, 512)
-
-# Create an instance of the BitMGQA model with input size 512, 8 attention heads, and 4 layers
-gqa = BitMGQA(512, 8, 4)
-
-# Pass the input tensor through the BitMGQA model and get the output and attention weights
-out, _ = gqa(x, x, x, need_weights=True)
+## Examples:
 
-# Print the shapes of the output tensor and attention tensor
-print(out)
-```
-
-### `BitFeedForward`
-- Feedforward as shown in the diagram with BitLinear and a GELU:
-- Linear -> GELU -> Linear
-- You can add dropouts, or layernorms, or other layers for a better ffn
-
-```python
-import torch
-from bitnet import BitFeedForward
-
-# Create a random input tensor of shape (10, 512)
-x = torch.randn(10, 512)
-
-# Create an instance of the BitFeedForward class with the following parameters:
-# - input_dim: 512
-# - hidden_dim: 512
-# - num_layers: 4
-# - swish: True (use Swish activation function)
-# - post_act_ln: True (apply Layer Normalization after each activation)
-# - dropout: 0.1 (apply dropout with a probability of 0.1)
-ff = BitFeedForward(512, 512, 4, swish=True, post_act_ln=True, dropout=0.1)
-
-# Apply the BitFeedForward network to the input tensor x
-y = ff(x)
-
-# Print the shape of the output tensor y
-print(y)  # torch.Size([10, 512])
-```
-
-## Inference
-```python
-from bitnet import BitNetInference
-
-bitnet = BitNetInference()
-bitnet.load_model("../model_checkpoint.pth")  # Download model
-output_str = bitnet.generate("The dog jumped over the ", 512)
-print(output_str)
-```
-
-## Huggingface Usage
-```python
-import torch
-from transformers import AutoModelForSequenceClassification, AutoTokenizer
-
-from bitnet import replace_linears_in_hf
-
-# Load a model from Hugging Face's Transformers
-model_name = "bert-base-uncased"
-tokenizer = AutoTokenizer.from_pretrained(model_name)
-model = AutoModelForSequenceClassification.from_pretrained(model_name)
-
-# Replace Linear layers with BitLinear
-replace_linears_in_hf(model)
-
-# Example text to classify
-text = "Replace this with your text"
-inputs = tokenizer(
-    text, return_tensors="pt", padding=True, truncation=True, max_length=512
-)
-
-# Perform inference
-model.eval()  # Set the model to evaluation mode
-with torch.no_grad():
-    outputs = model(**inputs)
-    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
-    print(predictions)
-
-# Process predictions
-predicted_class_id = predictions.argmax().item()
-print(f"Predicted class ID: {predicted_class_id}")
-
-# Optionally, map the predicted class ID to a label, if you know the classification labels
-# labels = ["Label 1", "Label 2", ...]  # Define your labels corresponding to the model's classes
-# print(f"Predicted label: {labels[predicted_class_id]}")
+1. **Feedforward** `MNIST`
+Train a one-hidden layer 1.58bits neural network on the MNIST dataset 
+```sh
+python examples/mnist_ff_example.py
 ```
 
 # License
 MIT
 
-# Citation
+# Citations
 ```bibtex
 @misc{2310.11453,
 Author = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei},
@@ -182,17 +65,10 @@ Year = {2023},
 Eprint = {arXiv:2310.11453},
 }
 
-```
-
-

+@misc{2402.17764,
+Author = {Shuming Ma Hongyu Wang∗ Lingxiao Ma Lei Wang Wenhui Wang Shaohan Huang Li Dong Ruiping Wang Jilong Xue Furu Wei},  
+Title = {The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits (or Maybe Not Quite)},
+Year = {2024},
+Eprint = {arXiv:2402.17764},
+}
+```
\ No newline at end of file
diff --git a/bitnet/at.py b/bitnet/at.py
deleted file mode 100644
index 13565ff..0000000
--- a/bitnet/at.py
+++ /dev/null
@@ -1,107 +0,0 @@
-import torch
-import torch.nn.functional as F
-from einops import rearrange
-from torch import nn
-
-# helper function
-
-
-def exists(val):
-    return val is not None
-
-
-def eval_decorator(fn):
-    def inner(model, *args, **kwargs):
-        was_training = model.training
-        model.eval()
-        out = fn(model, *args, **kwargs)
-        model.train(was_training)
-        return out
-
-    return inner
-
-
-# top k filtering
-
-
-def top_k(logits, thres=0.9):
-    k = int((1 - thres) * logits.shape[-1])
-    val, ind = torch.topk(logits, k)
-    probs = torch.full_like(logits, float("-inf"))
-    probs.scatter_(1, ind, val)
-    return probs
-
-
-class AutoregressiveWrapper(nn.Module):
-    """
-    AutoregressiveWrapper is a wrapper class that adds autoregressive generation functionality to a given neural network.
-
-    Args:
-        net (nn.Module): The neural network model.
-        max_seq_len (int): The maximum sequence length for generation. Defaults to 2048.
-        pad_value (int): The padding value for generated sequences. Defaults to 0.
-    """
-
-    def __init__(self, net, max_seq_len=2048, pad_value=0):
-        super().__init__()
-        self.max_seq_len = max_seq_len
-        self.pad_value = pad_value
-        self.net = net
-
-    @torch.no_grad()
-    @eval_decorator
-    def generate(
-        self,
-        start_tokens,
-        seq_len,
-        eos_token=None,
-        temperature=1.0,
-        filter_thres=0.9,
-        **kwargs,
-    ):
-        """
-        Generates autoregressive sequences based on the given start tokens.
-
-        Args:
-            start_tokens (torch.Tensor): The initial tokens to start the generation.
-            seq_len (int): The length of the generated sequence.
-            eos_token (int, optional): The end-of-sequence token. If provided, generation will stop when this token is generated. Defaults to None.
-            temperature (float, optional): The temperature value for controlling the randomness of the generation. Higher values result in more randomness. Defaults to 1.0.
-            filter_thres (float, optional): The threshold value for filtering logits during generation. Only logits above this threshold will be considered. Defaults to 0.9.
-            **kwargs: Additional keyword arguments to be passed to the underlying network.
-
-        Returns:
-            torch.Tensor: The generated sequence.
-        """
-
-        b, t, device = *start_tokens.shape, start_tokens.device
-
-        out = start_tokens
-
-        for _ in range(seq_len):
-            logits = self.net(out, **kwargs)[:, -1, :]
-
-            filtered_logits = top_k(logits, thres=filter_thres)
-            probs = F.softmax(filtered_logits / temperature, dim=-1)
-
-            sample = torch.multinomial(probs, 1)
-
-            out = torch.cat((out, sample), dim=-1)
-
-            if exists(eos_token):
-                is_eos_token = out == eos_token
-
-                if is_eos_token.any(dim=-1).all():
-                    # mask out everything after the eos tokens
-                    shifted_is_eos_tokens = F.pad(is_eos_token, (1, -1))
-                    mask = shifted_is_eos_tokens.float().cumsum(dim=-1) >= 1
-                    out = out.masked_fill(mask, self.pad_value)
-                    break
-
-        out = out[:, t:]
-        return out
-
-    def forward(self, x, **kwargs):
-        x_inp, x_labels = x[:, :-1], x[:, 1:]
-        logits = self.net(x_inp, **kwargs)
-        return F.cross_entropy(rearrange(logits, "b c n -> b n c"), x_labels)
-class BitFeedForward(nn.Module):
-    """
-    BitFeedForward module performs feed-forward operations on the input tensor.
-
-    Args:
-        dim (int): The input dimension.
-        dim_out (int, optional): The output dimension. If not provided, it is set to the input dimension.
-        mult (int, optional): The multiplier for the inner dimension. Default is 4.
-        glu (bool, optional): Whether to use Gated Linear Unit (GLU) activation. Default is False.
-        glu_mult_bias (bool, optional): Whether to apply bias to the GLU activation. Default is False.
-        swish (bool, optional): Whether to use Swish activation. Default is False.
-        relu_squared (bool, optional): Whether to use squared ReLU activation. Default is False.
-        post_act_ln (bool, optional): Whether to apply Layer Normalization after activation. Default is False.
-        dropout (float, optional): The dropout probability. Default is 0.0.
-        no_bias (bool, optional): Whether to exclude bias in linear layers. Default is False.
-        zero_init_output (bool, optional): Whether to initialize the last linear layer to 0. Default is False.
-    """
-
-    def __init__(
-        self,
-        dim: int,
-        dim_out: Optional[int] = None,
-        mult: int = 4,
-        glu: bool = False,
-        glu_mult_bias: bool = False,
-        swish: bool = False,
-        post_act_ln: bool = False,
-        dropout: float = 0.0,
-        no_bias: bool = False,
-        zero_init_output: bool = False,
-        *args,
-        **kwargs
-    ):
-        super().__init__()
-        inner_dim = int(dim * mult)
-        dim_out = default(dim_out, dim)
-
-        if swish:
-            activation = nn.SiLU()
-        else:
-            activation = nn.GELU()
-
-        if glu:
-            project_in = GLU(dim, inner_dim, activation, mult_bias=glu_mult_bias)
-        else:
-            project_in = nn.Sequential(
-                BitLinear(dim, inner_dim, bias=not no_bias, *args, **kwargs), activation
-            )
-
-        self.ff = nn.Sequential(
-            project_in,
-            nn.LayerNorm(inner_dim) if post_act_ln else None,
-            nn.Dropout(dropout),
-            BitLinear(inner_dim, dim_out, bias=not no_bias, *args, **kwargs),
-        )
-
-        # init last linear layer to 0
-        if zero_init_output:
-            init_zero_(self.ff[-1])
-
-    def forward(self, x):
-        """
-        Forward pass of the BitFeedForward module.
-
-        Args:
-            x (torch.Tensor): The input tensor.
-
-        Returns:
-            torch.Tensor: The output tensor.
-        """
-        return self.ff(x)
diff --git a/bitnet/bit_transformer.py b/bitnet/bit_transformer.py
deleted file mode 100644
index fc81860..0000000
--- a/bitnet/bit_transformer.py
+++ /dev/null
@@ -1,134 +0,0 @@
-import torch
-import torch.nn.functional as F
-from torch import Tensor
-from torch import nn
-
-from bitnet.bit_ffn import BitFeedForward
-from bitnet.bit_attention import BitMGQA
-
-
-def l2norm(t, dim=-1):
-    return F.normalize(t, dim=dim)
-
-
-class RMSNorm(nn.Module):
-    """
-    Root Mean Square Normalization (RMSNorm) module.
-
-    Args:
-        dim (int): The input dimension.
-        affine (bool, optional): If True, apply an affine transformation to the normalized output.
-            Default is True.
-
-    Attributes:
-        scale (float): The scaling factor for the normalized output.
-        gamma (torch.Tensor or float): The learnable parameter for the affine transformation.
-
-    """
-
-    def __init__(self, dim, affine=True):
-        super().__init__()
-        self.scale = dim**0.5
-        self.gamma = nn.Parameter(torch.ones(dim)) if affine else 1.0
-
-    def forward(self, x):
-        return l2norm(x) * self.gamma * self.scale
-
-
-class Transformer(nn.Module):
-    """
-    Transformer module that applies multi-head attention and feed-forward layers.
-
-    Args:
-        dim (int): The dimension of the input and output tensors.
-        heads (int): The number of attention heads.
-        depth (int): The number of transformer layers.
-        ff_mult (int, optional): The multiplier for the hidden dimension in the feed-forward layers.
-            Defaults to 2.
-        *args: Variable length argument list.
-        **kwargs: Arbitrary keyword arguments.
-
-    Attributes:
-        layers (nn.ModuleList): List of multi-head attention layers.
-        ffn_layers (nn.ModuleList): List of feed-forward layers.
-
-    """
-
-    def __init__(
-        self, dim: int, heads: int, depth: int, ff_mult: int = 2, *args, **kwargs
-    ):
-        super().__init__()
-        self.layers = nn.ModuleList([])
-        self.ffn_layers = nn.ModuleList([])
-
-        for _ in range(depth):
-            self.layers.append(BitMGQA(dim, heads, *args, **kwargs))
-
-            self.ffn_layers.append(
-                BitFeedForward(
-                    dim,
-                    dim,
-                    ff_mult,
-                    swish=True,
-                    post_act_ln=True,
-                    dropout=0.1,
-                ),
-            )
-
-    def forward(self, x: Tensor, *args, **kwargs) -> Tensor:
-        skip = x
-        for attn, ffn in zip(self.layers, self.ffn_layers):
-            x, _ = attn(x, x, x, is_causal=True, *args, **kwargs)
-            x = x + skip
-            x = ffn(x) + x
-        return x
-
-
-# [MAIN MODEL] BitNetTransformer
-class BitNetTransformer(nn.Module):
-    """
-    BitNetTransformer is a transformer-based model for BitNet.
-
-    Args:
-        dim (int): The dimension of the token embeddings.
-        depth (int): The number of transformer layers.
-        num_tokens (int): The number of tokens in the vocabulary.
-        heads (int, optional): The number of attention heads in the transformer. Defaults to 8.
-        ff_mult (int, optional): The multiplier for the feed-forward layer dimension. Defaults to 4.
-
-    Examples:
-    >>> import torch
-    >>> from bitnet import BitNetTransformer
-    >>> x = torch.randint(0, 20000, (1, 1024))
-    >>> bitnet = BitNetTransformer(
-    ...     num_tokens=20000,
-    ...     dim=1024,
-    ...     depth=6,
-    ...     heads=8,
-    ...     ff_mult=4,
-    ... )
-    >>> logits = bitnet(x)
-    >>> print(logits)
-    """
-
-    def __init__(
-        self,
-        dim: int,
-        depth: int,
-        num_tokens: int,
-        heads=8,
-        ff_mult=4,
-    ):
-        super().__init__()
-        self.emb = nn.Embedding(num_tokens, dim)
-
-        self.transformer = Transformer(
-            dim=dim, depth=depth, heads=heads, ff_mult=ff_mult
-        )
-
-        self.to_logits = nn.Sequential(RMSNorm(dim), nn.Linear(dim, num_tokens))
-
-    def forward(self, x):
-        x = self.emb(x)
-        x = self.transformer(x)
-        return self.to_logits(x)
diff --git a/bitnet/bitbnet_b158.py b/bitnet/bitbnet_b158.py
deleted file mode 100644
index 7239d55..0000000
--- a/bitnet/bitbnet_b158.py
+++ /dev/null
@@ -1,171 +0,0 @@
-import torch
-from torch import Tensor, nn
-
-
-def absmean_quantize_weights(weights):
-    """
-    Quantizes the weights to -1, 0, or +1 using an absmean quantization function.
-
-    Parameters:
-    - weights (Tensor): The weights of a neural network layer.
-
-    Returns:
-    - Tensor: The quantized weights.
-    """
-    # Calculate the average absolute value (γ) of the weights
-    gamma = torch.mean(torch.abs(weights))
-    
-    # Scale weights by γ and round to the nearest integer among {-1, 0, +1}
-    quantized_weights = torch.clamp(torch.round(weights / gamma), min=-1, max=1)
-    
-    return quantized_weights
-
-
-
-class BitLinear(nn.Linear):
-    """
-    BitLinear is a custom linear layer that performs binarization of weights and quantization of activations
-    in a group-wise manner.
-
-    Args:
-        in_features (int): Number of input features.
-        out_features (int): Number of output features.
-        bias (bool, optional): If set to False, the layer will not learn an additive bias. Default is True.
-        num_groups (int, optional): Number of groups to divide the weights and activations into. Default is 1.
-    """
-
-    def __init__(
-        self,
-        in_features: int,
-        out_features: int,
-        bias: bool = True,
-        num_groups: int = 1,
-        b: int = 8,
-    ):
-        super().__init__(in_features, out_features, bias)
-        self.in_features = in_features
-        self.out_features = out_features
-        self.b = b
-        self.num_groups = num_groups
-        self.eps = 1e-5
-        self.norm = nn.LayerNorm(in_features)
-
-    def ste(self, x):
-        """
-        Applies the sign function for binarization and uses Straight-Through Estimator (STE) during backward pass.
-
-        Args:
-            x (Tensor): Input tensor.
-
-        Returns:
-            Tensor: Binarized tensor.
-        """
-        binarized_x = torch.sign(x)
-        binarized_x = (binarized_x - x).detach() + x
-        return binarized_x
-
-    def binarize_weights_groupwise(self):
-        """
-        Binarizes the weights of the layer in a group-wise manner using STE.
-
-        Returns:
-            Tensor: Binarized weights tensor.
-        """
-        group_size = self.weight.shape[0] // self.num_groups
-        binarized_weights = torch.zeros_like(self.weight)
-
-        for g in range(self.num_groups):
-            start_idx = g * group_size
-            end_idx = (g + 1) * group_size
-            weight_group = self.weight[start_idx:end_idx]
-
-            alpha_g = weight_group.mean()
-            binarized_weights[start_idx:end_idx] = self.ste(weight_group - alpha_g)
-
-        return binarized_weights
-
-    def quantize_activations_groupwise(self, x):
-        """
-        Quantizes the activations of the layer in a group-wise manner.
-
-        Args:
-            x (Tensor): Input tensor.
-            b (int, optional): Number of bits for quantization. Default is 8.
-
-        Returns:
-            Tensor: Quantized activations tensor.
-        """
-        Q_b = 2 ** (self.b - 1)
-
-        group_size = x.shape[0] // self.num_groups
-        quantized_x = torch.zeros_like(x)
-
-        for g in range(self.num_groups):
-            start_idx = g * group_size
-            end_idx = (g + 1) * group_size
-            activation_group = x[start_idx:end_idx]
-
-            gamma_g = activation_group.abs().max()
-            quantized_x[start_idx:end_idx] = torch.clamp(
-                activation_group * Q_b / (gamma_g + self.eps),
-                -Q_b + self.eps,
-                Q_b - self.eps,
-            )
-
-        return quantized_x
-    
-    def dequantize_activations_groupwise(self, x):
-        """
-        Dequantizes the activations of the layer in a group-wise manner.
-
-        Args:
-            x (Tensor): Quantized input tensor.
-            b (int, optional): Number of bits used during the quantization. Default is 8.
-
-        Returns:
-            Tensor: Dequantized activations tensor.
-        """
-        Q_b = 2 ** (self.b - 1)
-        dequantized_x = torch.zeros_like(x)
-        for g in range(self.num_groups):
-            start_idx = g * x.shape[0] // self.num_groups
-            end_idx = (g + 1) * x.shape[0] // self.num_groups
-            quantized_group = x[start_idx:end_idx]
-            gamma_g = quantized_group.abs().max()
-            dequantized_x[start_idx:end_idx] = quantized_group * gamma_g / Q_b
-        return dequantized_x
-
-    def forward(self, x: Tensor) -> Tensor:
-        """
-        Forward pass of the BitLinear layer.
-
-        Args:
-            x (Tensor): Input tensor.
-
-        Returns:
-            Tensor: Output tensor.
-        """
-        # Normalize input
-        x = self.norm(x)
-
-        # Binarize weights and quantize activations
-        binarized_weights = self.binarize_weights_groupwise()
-
-        # Perform linear transformation
-        output = torch.nn.functional.linear(x, binarized_weights, self.bias)
-
-        # Quantize activations
-        output = absmean_quantize_weights(output)
-        
-        # Dequantize activations
-        # output = self.dequantize_activations_groupwise(output)
-
-        # Return output
-        return output
-
-
-# Example usage
-bitlinear = BitLinear(10, 5, num_groups=2, b=8)
-input_tensor = torch.randn(5, 10)  # Example input tensor
-output = bitlinear(input_tensor)
-print(output)  # Example output tensor
diff --git a/bitnet/inference.py b/bitnet/inference.py
deleted file mode 100644
index 3c2b5ad..0000000
--- a/bitnet/inference.py
+++ /dev/null
@@ -1,65 +0,0 @@
-import numpy as np
-import torch
-
-from bitnet.at import AutoregressiveWrapper
-from bitnet.bit_transformer import BitNetTransformer
-
-
-class BitNetInference:
-    """
-    A class used to perform inference with the BitNetTransformer model.
-
-    ...
-
-    Attributes
-    ----------
-    model : torch.nn.Module
-        an instance of the BitNetTransformer model
-    device : str
-        the device to run the model on ('cpu' or 'cuda')
-
-    Methods
-    -------
-    load_model(model_path)
-        Loads a trained model from a .pth file.
-    generate(input_str, length)
-        Generates a sequence of tokens based on the input string.
-    """
-
-    def __init__(self, device="cuda"):
-        """
-        Parameters
-        ----------
-        device : str, optional
-            The device to run the model on ('cpu' or 'cuda'). By default, 'cuda' is used.
-        """
-        self.device = device
-        self.model = BitNetTransformer(num_tokens=256, dim=512, depth=8)
-        self.model = AutoregressiveWrapper(self.model, max_seq_len=1024)
-        self.model.to(self.device)
-
-    def load_model(self, model_path):
-        """Loads a trained model from a .pth file."""
-        self.model.load_state_dict(torch.load(model_path, weights_only=True))
-        self.model.eval()
-
-    @staticmethod
-    def decode_token(token):
-        """Decodes a token into a character."""
-        return str(chr(max(32, token)))
-
-    @staticmethod
-    def decode_tokens(tokens):
-        """Decodes a sequence of tokens into a string."""
-        return "".join(list(map(BitNetInference.decode_token, tokens)))
-
-    def generate(self, input_str, length):
-        """Generates a sequence of tokens based on the input string."""
-        inp = (
-            torch.from_numpy(np.fromstring(input_str, dtype=np.uint8))
-            .long()
-            .to(self.device)
-        )
-        sample = self.model.generate(inp[None, ...], length)
-        output_str = self.decode_tokens(sample[0])
-        return output_str
diff --git a/example.py b/example.py
deleted file mode 100644
index 5c481ee..0000000
--- a/example.py
+++ /dev/null
@@ -1,14 +0,0 @@
-import torch
-
-from bitnet import BitLinear
-
-# Input
-x = torch.randn(10, 512)
-
-# BitLinear layer
-layer = BitLinear(512, 400)
-
-# Output
-y = layer(x)
-
-print(y)
diff --git a/mnist_example.py b/examples/mnist_ff_example.py
similarity index 91%
rename from mnist_example.py
rename to examples/mnist_ff_example.py
index a0963fb..363e43d 100644
--- a/mnist_example.py
+++ b/examples/mnist_ff_example.py
@@ -18,8 +18,8 @@ transform = transforms.Compose([
 ])
 
 # Load MNIST dataset
-train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
-test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)
+train_dataset = datasets.MNIST('./mnist_data', train=True, download=True, transform=transform)
+test_dataset = datasets.MNIST('./mnist_data', train=False, download=True, transform=transform)
 
 # Create data loaders
 train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
diff --git a/huggingface_example.py b/huggingface_example.py
deleted file mode 100644
index e19f959..0000000
--- a/huggingface_example.py
+++ /dev/null
@@ -1,33 +0,0 @@
-import torch
-from transformers import AutoModelForSequenceClassification, AutoTokenizer
-
-from bitnet import replace_linears_in_hf
-
-# Load a model from Hugging Face's Transformers
-model_name = "bert-base-uncased"
-tokenizer = AutoTokenizer.from_pretrained(model_name)
-model = AutoModelForSequenceClassification.from_pretrained(model_name)
-
-# Replace Linear layers with BitLinear
-replace_linears_in_hf(model)
-
-# Example text to classify
-text = "Replace this with your text"
-inputs = tokenizer(
-    text, return_tensors="pt", padding=True, truncation=True, max_length=512
-)
-
-# Perform inference
-model.eval()  # Set the model to evaluation mode
-with torch.no_grad():
-    outputs = model(**inputs)
-    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
-    print(predictions)
-
-# Process predictions
-predicted_class_id = predictions.argmax().item()
-print(f"Predicted class ID: {predicted_class_id}")
-
-# Optionally, map the predicted class ID to a label, if you know the classification labels
-# labels = ["Label 1", "Label 2", ...]  # Define your labels corresponding to the model's classes
-# print(f"Predicted label: {labels[predicted_class_id]}")
diff --git a/pyproject.toml b/pyproject.toml
deleted file mode 100644
index 21b938c..0000000
--- a/pyproject.toml
+++ /dev/null
@@ -1,47 +0,0 @@
-[build-system]
-requires = ["poetry-core>=1.0.0"]
-build-backend = "poetry.core.masonry.api"
-
-[tool.poetry]
-name = "bitnet"
-version = "0.1.4"
-description = "bitnet - Pytorch"
-license = "MIT"
-authors = ["Kye Gomez <kye@apac.ai>"]
-homepage = "https://github.com/kyegomez/bitnet"
-documentation = "https://github.com/kyegomez/bitnet"  # Add this if you have documentation.
-readme = "README.md"  # Assuming you have a README.md
-repository = "https://github.com/kyegomez/bitnet"
-keywords = ["artificial intelligence", "deep learning", "optimizers", "Prompt Engineering"]
-classifiers = [
-    "Development Status :: 4 - Beta",
-    "Intended Audience :: Developers",
-    "Topic :: Scientific/Engineering :: Artificial Intelligence",
-    "License :: OSI Approved :: MIT License",
-    "Programming Language :: Python :: 3.6"
-]
-
-[tool.poetry.dependencies]
-python = "^3.6"
-torch = "*"
-einops = "*"
-zetascale = "2.1.6"
-
-
-
-[tool.poetry.group.lint.dependencies]
-ruff = "^0.0.249"
-types-toml = "^0.10.8.1"
-types-redis = "^4.3.21.6"
-types-pytz = "^2023.3.0.0"
-black = "^23.1.0"
-types-chardet = "^5.0.4.6"
-mypy-protobuf = "^3.0.0"
-
-
-[tool.autopep8]
-max_line_length = 120
-ignore = "E501,W6"  # or ["E501", "W6"]
-in-place = true
-recursive = true
-aggressive = 3
\ No newline at end of file
diff --git a/requirements.txt b/requirements.txt
index 58c0cc3..37f700a 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,3 +1,2 @@
 torch
-einops
-zetascale==2.1.6
\ No newline at end of file
+torchvision
\ No newline at end of file
diff --git a/setup.py b/setup.py
new file mode 100644
index 0000000..6f6c7b9
--- /dev/null
+++ b/setup.py
@@ -0,0 +1,25 @@
+#!/usr/bin/env python3
+
+from setuptools import setup
+
+# Function to read the requirements from 'requirements.txt'
+def read_requirements():
+    with open('requirements.txt', 'r') as req:
+        return req.read().splitlines()
+
+setup(name='bitnet',
+      version='0.0.0',
+      description="Neural Netws can be as performant as needed even with 1.58 bits",
+      author='Dario Cazzani, Aleks Yeganov',
+      packages=['bitnet'],
+      classifiers=[
+        "TBD"
+      ],
+      install_requires=read_requirements(),
+      python_requires='>=3.11',
+      extras_require={
+        'testing': [
+            "pytest",
+        ],
+      },
+      include_package_data=True)
\ No newline at end of file
diff --git a/tests/test_bitffn.py b/tests/test_bitffn.py
deleted file mode 100644
index ecee77c..0000000
--- a/tests/test_bitffn.py
+++ /dev/null
@@ -1,23 +0,0 @@
-import torch
-from torch import nn
-
-from bitnet.bit_ffn import BitFeedForward
-from bitnet.bitlinear import BitLinear
-
-
-def test_bitfeedforward_initialization():
-    bitffn = BitFeedForward(dim=512, ff_mult=4)
-    assert isinstance(bitffn.layer, nn.Sequential)
-    assert len(bitffn.layer) == 3
-    assert isinstance(bitffn.layer[0], BitLinear)
-    assert isinstance(bitffn.layer[1], nn.GELU)
-    assert isinstance(bitffn.layer[2], BitLinear)
-    assert bitffn.layer[0].out_features == 512 * 4
-    assert bitffn.layer[2].in_features == 512 * 4
-
-
-def test_bitfeedforward_forward_pass():
-    bitffn = BitFeedForward(dim=512, ff_mult=4)
-    x = torch.randn(1, 512)
-    out = bitffn(x)
-    assert out.shape == x.shape
diff --git a/tests/test_bitlinear.py b/tests/test_bitlinear.py
index b7587c2..61a86f2 100644
--- a/tests/test_bitlinear.py
+++ b/tests/test_bitlinear.py
@@ -9,8 +9,6 @@ def test_bitlinear_initialization():
     assert bitlinear.out_features == 256
     assert bitlinear.weight.shape == (256, 512)
     assert bitlinear.bias.shape == (256,)
-    assert bitlinear.gamma.shape == (512,)
-    assert bitlinear.beta.shape == (256,)
 
 
 def test_bitlinear_forward_pass():
@@ -22,12 +20,4 @@ def test_bitlinear_forward_pass():
 
 def test_bitlinear_no_bias():
     bitlinear = BitLinear(in_features=512, out_features=256, bias=False)
-    assert bitlinear.bias is None
-
-
-def test_bitlinear_quantization():
-    bitlinear = BitLinear(in_features=512, out_features=256, bias=True)
-    x = torch.randn(1, 512)
-    out = bitlinear(x)
-    assert torch.all(out <= bitlinear.beta.unsqueeze(0).expand_as(out))
-    assert torch.all(out >= -bitlinear.beta.unsqueeze(0).expand_as(out))
+    assert bitlinear.bias is None
\ No newline at end of file
diff --git a/tests/test_transformer.py b/tests/test_transformer.py
deleted file mode 100644
index 376d5d8..0000000
--- a/tests/test_transformer.py
+++ /dev/null
@@ -1,18 +0,0 @@
-import torch
-
-from bitnet.bit_transformer import BitFeedForward, BitNetTransformer, MultiheadAttention
-
-
-def test_bitnet_transformer_initialization():
-    bitnet = BitNetTransformer(num_tokens=20000, dim=512, heads=8, depth=6, ff_mult=4)
-    assert len(bitnet.layers) == 6
-    assert len(bitnet.ffn_layers) == 6
-    assert all(isinstance(layer, MultiheadAttention) for layer in bitnet.layers)
-    assert all(isinstance(layer, BitFeedForward) for layer in bitnet.ffn_layers)
-
-
-def test_bitnet_transformer_forward_pass():
-    bitnet = BitNetTransformer(num_tokens=20000, dim=512, heads=8, depth=6, ff_mult=4)
-    x = torch.randn(1, 100, 512)
-    out = bitnet(x)
-    assert out.shape == x.shape
diff --git a/tests/tests.py b/tests/tests.py
index e4ea383..ea09052 100644
--- a/tests/tests.py
+++ b/tests/tests.py
@@ -1,288 +1,288 @@
-import pytest
-import torch
-from torch.nn import functional as F
+# import pytest
+# import torch
+# from torch.nn import functional as F
 
-from bitnet.bitlinear import BitLinear, absmax_quantize
-from bitnet.bit_transformer import (
-    BitNetTransformer,
-    ParallelTransformerBlock,
-    Transformer,
-)
+# from bitnet.bitlinear import BitLinear, absmax_quantize
+# from bitnet.bit_transformer import (
+#     BitNetTransformer,
+#     ParallelTransformerBlock,
+#     Transformer,
+# )
 
-# Basic Tests:
+# # Basic Tests:
 
 
-def test_absmax_quantize():
-    tensor = torch.tensor([1.5, -2.0, 3.0, -4.0])
-    quant, dequant = absmax_quantize(tensor)
-    assert quant.dtype == torch.int8
-    assert torch.allclose(dequant, tensor, atol=1e-2)
+# def test_absmax_quantize():
+#     tensor = torch.tensor([1.5, -2.0, 3.0, -4.0])
+#     quant, dequant = absmax_quantize(tensor)
+#     assert quant.dtype == torch.int8
+#     assert torch.allclose(dequant, tensor, atol=1e-2)
 
 
-def test_bitlinear_initialization():
-    layer = BitLinear(10, 20)
-    assert layer.in_features == 10
-    assert layer.out_features == 20
-    assert layer.weight.shape == (20, 10)
+# def test_bitlinear_initialization():
+#     layer = BitLinear(10, 20)
+#     assert layer.in_features == 10
+#     assert layer.out_features == 20
+#     assert layer.weight.shape == (20, 10)
 
 
-def test_bitlinear_forward():
-    layer = BitLinear(10, 20)
-    input_tensor = torch.randn(5, 10)
-    output = layer(input_tensor)
-    assert output.shape == (5, 20)
+# def test_bitlinear_forward():
+#     layer = BitLinear(10, 20)
+#     input_tensor = torch.randn(5, 10)
+#     output = layer(input_tensor)
+#     assert output.shape == (5, 20)
 
 
-# Fixtures:
+# # Fixtures:
 
 
-@pytest.fixture
-def random_tensor():
-    return torch.randn(5, 10)
+# @pytest.fixture
+# def random_tensor():
+#     return torch.randn(5, 10)
 
 
-# Parameterized Testing:
+# # Parameterized Testing:
 
 
-@pytest.mark.parametrize("bits", [4, 8, 12, 16])
-def test_absmax_quantize_bits(random_tensor, bits):
-    quant, dequant = absmax_quantize(random_tensor, bits=bits)
-    assert quant.dtype == torch.int8
-    assert torch.allclose(dequant, random_tensor, atol=1e-2)
+# @pytest.mark.parametrize("bits", [4, 8, 12, 16])
+# def test_absmax_quantize_bits(random_tensor, bits):
+#     quant, dequant = absmax_quantize(random_tensor, bits=bits)
+#     assert quant.dtype == torch.int8
+#     assert torch.allclose(dequant, random_tensor, atol=1e-2)
 
 
-# More Tests for BitLinear:
+# # More Tests for BitLinear:
 
 
-@pytest.mark.parametrize(
-    "in_features,out_features", [(10, 20), (20, 40), (5, 10), (15, 10)]
-)
-def test_bitlinear_shapes(in_features, out_features):
-    layer = BitLinear(in_features, out_features)
-    assert layer.weight.shape == (out_features, in_features)
+# @pytest.mark.parametrize(
+#     "in_features,out_features", [(10, 20), (20, 40), (5, 10), (15, 10)]
+# )
+# def test_bitlinear_shapes(in_features, out_features):
+#     layer = BitLinear(in_features, out_features)
+#     assert layer.weight.shape == (out_features, in_features)
 
 
-@pytest.mark.parametrize("groups", [1, 2, 5])
-def test_bitlinear_groups(groups):
-    layer = BitLinear(10, 20, groups=groups)
-    assert layer.groups == groups
+# @pytest.mark.parametrize("groups", [1, 2, 5])
+# def test_bitlinear_groups(groups):
+#     layer = BitLinear(10, 20, groups=groups)
+#     assert layer.groups == groups
 
 
-def test_bitlinear_reset_parameters():
-    layer = BitLinear(10, 20)
-    original_weights = layer.weight.clone()
-    layer.reset_parameters()
-    assert not torch.equal(original_weights, layer.weight)
+# def test_bitlinear_reset_parameters():
+#     layer = BitLinear(10, 20)
+#     original_weights = layer.weight.clone()
+#     layer.reset_parameters()
+#     assert not torch.equal(original_weights, layer.weight)
 
 
-@pytest.mark.parametrize("groups", [1, 2, 5])
-def test_bitlinear_forward_with_groups(random_tensor, groups):
-    layer = BitLinear(10, 20, groups=groups)
-    output = layer(random_tensor)
-    assert output.shape == (5, 20)
+# @pytest.mark.parametrize("groups", [1, 2, 5])
+# def test_bitlinear_forward_with_groups(random_tensor, groups):
+#     layer = BitLinear(10, 20, groups=groups)
+#     output = layer(random_tensor)
+#     assert output.shape == (5, 20)
 
 
-def test_bitlinear_zero_input():
-    layer = BitLinear(10, 20)
-    input_tensor = torch.zeros(5, 10)
-    output = layer(input_tensor)
-    assert torch.allclose(output, torch.zeros(5, 20), atol=1e-2)
+# def test_bitlinear_zero_input():
+#     layer = BitLinear(10, 20)
+#     input_tensor = torch.zeros(5, 10)
+#     output = layer(input_tensor)
+#     assert torch.allclose(output, torch.zeros(5, 20), atol=1e-2)
 
 
-def test_bitlinear_weight_sign():
-    layer = BitLinear(10, 20)
-    input_tensor = torch.randn(5, 10)
-    output_before = layer(input_tensor)
-    layer.weight.data = torch.abs(layer.weight.data)
-    output_after = layer(input_tensor)
-    assert not torch.allclose(output_before, output_after)
+# def test_bitlinear_weight_sign():
+#     layer = BitLinear(10, 20)
+#     input_tensor = torch.randn(5, 10)
+#     output_before = layer(input_tensor)
+#     layer.weight.data = torch.abs(layer.weight.data)
+#     output_after = layer(input_tensor)
+#     assert not torch.allclose(output_before, output_after)
 
 
-@pytest.mark.parametrize("groups", [1, 2, 5])
-def test_bitlinear_weight_group_normalization(groups):
-    layer = BitLinear(10, 20, groups=groups)
-    weight = layer.weight.view(groups, -1)
-    mean = weight.mean(dim=1, keepdim=True)
-    assert torch.allclose(mean, torch.zeros_like(mean), atol=1e-2)
+# @pytest.mark.parametrize("groups", [1, 2, 5])
+# def test_bitlinear_weight_group_normalization(groups):
+#     layer = BitLinear(10, 20, groups=groups)
+#     weight = layer.weight.view(groups, -1)
+#     mean = weight.mean(dim=1, keepdim=True)
+#     assert torch.allclose(mean, torch.zeros_like(mean), atol=1e-2)
 
 
-def test_bitlinear_weight_group_scaling():
-    layer = BitLinear(10, 20, groups=5)
-    weight = layer.weight.view(layer.groups, -1)
-    beta = torch.abs(weight).sum(dim=1, keepdim=True) / (
-        weight.shape[0] * weight.shape[1]
-    )
-    scaled_weight = weight * beta
-    assert torch.allclose(scaled_weight, layer.weight.view(20, 10))
+# def test_bitlinear_weight_group_scaling():
+#     layer = BitLinear(10, 20, groups=5)
+#     weight = layer.weight.view(layer.groups, -1)
+#     beta = torch.abs(weight).sum(dim=1, keepdim=True) / (
+#         weight.shape[0] * weight.shape[1]
+#     )
+#     scaled_weight = weight * beta
+#     assert torch.allclose(scaled_weight, layer.weight.view(20, 10))
 
 
-def test_bitlinear_input_quantization(random_tensor):
-    layer = BitLinear(10, 20)
-    quant_input, _ = absmax_quantize(random_tensor)
-    output = layer(quant_input.float())
-    assert output.shape == (5, 20)
+# def test_bitlinear_input_quantization(random_tensor):
+#     layer = BitLinear(10, 20)
+#     quant_input, _ = absmax_quantize(random_tensor)
+#     output = layer(quant_input.float())
+#     assert output.shape == (5, 20)
 
 
-# ... Continue adding more tests ...
-# - Test the forward pass with extreme input values.
-# - Test with different types of input tensors (e.g., int8, float16).
-# - Test the forward pass with batch sizes other than 5.
-# - Verify that using different initializations produces different results.
-# - Test the weight and input interactions during the forward pass.
-# - And many more...
+# # ... Continue adding more tests ...
+# # - Test the forward pass with extreme input values.
+# # - Test with different types of input tensors (e.g., int8, float16).
+# # - Test the forward pass with batch sizes other than 5.
+# # - Verify that using different initializations produces different results.
+# # - Test the weight and input interactions during the forward pass.
+# # - And many more...
 
-# ================================ Transformer with bitlinear ================================
+# # ================================ Transformer with bitlinear ================================
 
-
-@pytest.fixture
-def random_tensor():
-    """A fixture to generate a random tensor"""
-    return torch.randn(32, 512)
-
-
-@pytest.fixture
-def bitnet_model():
-    """A fixture to create an instance of BitNetTransformer model"""
-    return BitNetTransformer(
-        num_tokens=20000,
-        dim=512,
-        depth=6,
-        dim_head=64,
-        heads=8,
-        ff_mult=4,
-    )
-
-
-@pytest.mark.parametrize(
-    "dim, dim_head, heads, ff_mult",
-    [
-        (512, 64, 8, 4),
-        (256, 32, 4, 2),
-        (128, 16, 2, 1),
-    ],
-)
-def test_parallel_transformer_block(dim, dim_head, heads, ff_mult, random_tensor):
-    block = ParallelTransformerBlock(dim, dim_head, heads, ff_mult)
-    output = block(random_tensor)
-    assert output.shape == random_tensor.shape
-
-
-@pytest.mark.parametrize(
-    "dim, depth, heads, dim_head, ff_mult",
-    [
-        (512, 6, 8, 64, 4),
-        (256, 3, 4, 32, 2),
-        (128, 2, 2, 16, 1),
-    ],
-)
-def test_transformer(dim, depth, heads, dim_head, ff_mult, random_tensor):
-    transformer = Transformer(dim, depth, heads, dim_head, ff_mult)
-    output = transformer(random_tensor)
-    assert output.shape == random_tensor.shape
-
-
-def test_bitnet_transformer_forward(bitnet_model):
-    tokens = torch.randint(0, 20000, (1, 512))
-    logits = bitnet_model(tokens)
-    assert logits.shape == (1, 20000)
-
-
-def test_parallel_transformer_block_masking(random_tensor):
-    block = ParallelTransformerBlock(512, 64, 8, 4)
-    mask1 = block.get_mask(100, random_tensor.device)
-    mask2 = block.get_mask(200, random_tensor.device)
-    assert mask1.shape == (100, 100)
-    assert mask2.shape == (200, 200)
-
-
-def test_bitnet_transformer_embed(bitnet_model):
-    tokens = torch.randint(0, 20000, (1, 512))
-    embedded = bitnet_model.emb(tokens)
-    assert embedded.shape == (1, 512, 512)
-
-
-@pytest.mark.parametrize(
-    "dim, dim_head, heads, ff_mult",
-    [
-        (512, 64, 8, 4),
-        (256, 32, 4, 2),
-        (128, 16, 2, 1),
-    ],
-)
-def test_parallel_transformer_block_raises_for_incorrect_input(
-    dim, dim_head, heads, ff_mult
-):
-    block = ParallelTransformerBlock(dim, dim_head, heads, ff_mult)
-    with pytest.raises(Exception):
-        block(torch.randn(32, 100))
-
-
-@pytest.mark.parametrize(
-    "batch_size, seq_len",
-    [
-        (1, 512),
-        (32, 128),
-        (64, 256),
-    ],
-)
-def test_bitnet_transformer_for_various_input_shapes(bitnet_model, batch_size, seq_len):
-    tokens = torch.randint(0, 20000, (batch_size, seq_len))
-    logits = bitnet_model(tokens)
-    assert logits.shape == (batch_size, 20000)
-
-
-def test_rotary_embedding(bitnet_model, random_tensor):
-    block = ParallelTransformerBlock(512, 64, 8, 4)
-    rotary_emb1 = block.get_rotary_embedding(100, random_tensor.device)
-    rotary_emb2 = block.get_rotary_embedding(200, random_tensor.device)
-    assert rotary_emb1.shape == (100, 64)
-    assert rotary_emb2.shape == (200, 64)
-
-
-@pytest.mark.parametrize("mask_value", [100, 200, 300])
-def test_mask_persistency(random_tensor, mask_value):
-    block = ParallelTransformerBlock(512, 64, 8, 4)
-    block.get_mask(mask_value, random_tensor.device)
-    assert block.mask.shape[0] == mask_value
-
-
-@pytest.mark.parametrize(
-    "input_value, expected_output_shape",
-    [
-        (torch.randint(0, 20000, (1, 512)), (1, 20000)),
-        (torch.randint(0, 20000, (32, 256)), (32, 20000)),
-    ],
-)
-def test_bitnet_transformer_output_shapes(
-    bitnet_model, input_value, expected_output_shape
-):
-    logits = bitnet_model(input_value)
-    assert logits.shape == expected_output_shape
-
-
-def test_exceptions_on_wrong_dtype():
-    block = ParallelTransformerBlock(512, 64, 8, 4)
-    with pytest.raises(Exception):
-        block(torch.randn(32, 512).int())
-
-
-def test_bitnet_transformer_logit_values(bitnet_model):
-    tokens = torch.randint(0, 20000, (1, 512))
-    logits = bitnet_model(tokens)
-    probs = F.softmax(logits, dim=-1)
-    assert torch.allclose(probs.sum(dim=-1), torch.tensor(1.0))
-
-
-# Mocking and Monkeypatching
-
-
-def test_mocking_get_mask(monkeypatch, random_tensor):
-    mock_mask = torch.zeros(100, 100)
-    monkeypatch.setattr(
-        ParallelTransformerBlock, "get_mask", lambda self, n, device: mock_mask
-    )
-    block = ParallelTransformerBlock(512, 64, 8, 4)
-    assert torch.equal(block.get_mask(100, random_tensor.device), mock_mask)
-
-
-# Add more tests based on the scenarios and edge cases you want to cover.
+
+# @pytest.fixture
+# def random_tensor():
+#     """A fixture to generate a random tensor"""
+#     return torch.randn(32, 512)
+
+
+# @pytest.fixture
+# def bitnet_model():
+#     """A fixture to create an instance of BitNetTransformer model"""
+#     return BitNetTransformer(
+#         num_tokens=20000,
+#         dim=512,
+#         depth=6,
+#         dim_head=64,
+#         heads=8,
+#         ff_mult=4,
+#     )
+
+
+# @pytest.mark.parametrize(
+#     "dim, dim_head, heads, ff_mult",
+#     [
+#         (512, 64, 8, 4),
+#         (256, 32, 4, 2),
+#         (128, 16, 2, 1),
+#     ],
+# )
+# def test_parallel_transformer_block(dim, dim_head, heads, ff_mult, random_tensor):
+#     block = ParallelTransformerBlock(dim, dim_head, heads, ff_mult)
+#     output = block(random_tensor)
+#     assert output.shape == random_tensor.shape
+
+
+# @pytest.mark.parametrize(
+#     "dim, depth, heads, dim_head, ff_mult",
+#     [
+#         (512, 6, 8, 64, 4),
+#         (256, 3, 4, 32, 2),
+#         (128, 2, 2, 16, 1),
+#     ],
+# )
+# def test_transformer(dim, depth, heads, dim_head, ff_mult, random_tensor):
+#     transformer = Transformer(dim, depth, heads, dim_head, ff_mult)
+#     output = transformer(random_tensor)
+#     assert output.shape == random_tensor.shape
+
+
+# def test_bitnet_transformer_forward(bitnet_model):
+#     tokens = torch.randint(0, 20000, (1, 512))
+#     logits = bitnet_model(tokens)
+#     assert logits.shape == (1, 20000)
+
+
+# def test_parallel_transformer_block_masking(random_tensor):
+#     block = ParallelTransformerBlock(512, 64, 8, 4)
+#     mask1 = block.get_mask(100, random_tensor.device)
+#     mask2 = block.get_mask(200, random_tensor.device)
+#     assert mask1.shape == (100, 100)
+#     assert mask2.shape == (200, 200)
+
+
+# def test_bitnet_transformer_embed(bitnet_model):
+#     tokens = torch.randint(0, 20000, (1, 512))
+#     embedded = bitnet_model.emb(tokens)
+#     assert embedded.shape == (1, 512, 512)
+
+
+# @pytest.mark.parametrize(
+#     "dim, dim_head, heads, ff_mult",
+#     [
+#         (512, 64, 8, 4),
+#         (256, 32, 4, 2),
+#         (128, 16, 2, 1),
+#     ],
+# )
+# def test_parallel_transformer_block_raises_for_incorrect_input(
+#     dim, dim_head, heads, ff_mult
+# ):
+#     block = ParallelTransformerBlock(dim, dim_head, heads, ff_mult)
+#     with pytest.raises(Exception):
+#         block(torch.randn(32, 100))
+
+
+# @pytest.mark.parametrize(
+#     "batch_size, seq_len",
+#     [
+#         (1, 512),
+#         (32, 128),
+#         (64, 256),
+#     ],
+# )
+# def test_bitnet_transformer_for_various_input_shapes(bitnet_model, batch_size, seq_len):
+#     tokens = torch.randint(0, 20000, (batch_size, seq_len))
+#     logits = bitnet_model(tokens)
+#     assert logits.shape == (batch_size, 20000)
+
+
+# def test_rotary_embedding(bitnet_model, random_tensor):
+#     block = ParallelTransformerBlock(512, 64, 8, 4)
+#     rotary_emb1 = block.get_rotary_embedding(100, random_tensor.device)
+#     rotary_emb2 = block.get_rotary_embedding(200, random_tensor.device)
+#     assert rotary_emb1.shape == (100, 64)
+#     assert rotary_emb2.shape == (200, 64)
+
+
+# @pytest.mark.parametrize("mask_value", [100, 200, 300])
+# def test_mask_persistency(random_tensor, mask_value):
+#     block = ParallelTransformerBlock(512, 64, 8, 4)
+#     block.get_mask(mask_value, random_tensor.device)
+#     assert block.mask.shape[0] == mask_value
+
+
+# @pytest.mark.parametrize(
+#     "input_value, expected_output_shape",
+#     [
+#         (torch.randint(0, 20000, (1, 512)), (1, 20000)),
+#         (torch.randint(0, 20000, (32, 256)), (32, 20000)),
+#     ],
+# )
+# def test_bitnet_transformer_output_shapes(
+#     bitnet_model, input_value, expected_output_shape
+# ):
+#     logits = bitnet_model(input_value)
+#     assert logits.shape == expected_output_shape
+
+
+# def test_exceptions_on_wrong_dtype():
+#     block = ParallelTransformerBlock(512, 64, 8, 4)
+#     with pytest.raises(Exception):
+#         block(torch.randn(32, 512).int())
+
+
+# def test_bitnet_transformer_logit_values(bitnet_model):
+#     tokens = torch.randint(0, 20000, (1, 512))
+#     logits = bitnet_model(tokens)
+#     probs = F.softmax(logits, dim=-1)
+#     assert torch.allclose(probs.sum(dim=-1), torch.tensor(1.0))
+
+
+# # Mocking and Monkeypatching
+
+
+# def test_mocking_get_mask(monkeypatch, random_tensor):
+#     mock_mask = torch.zeros(100, 100)
+#     monkeypatch.setattr(
+#         ParallelTransformerBlock, "get_mask", lambda self, n, device: mock_mask
+#     )
+#     block = ParallelTransformerBlock(512, 64, 8, 4)
+#     assert torch.equal(block.get_mask(100, random_tensor.device), mock_mask)
+
+
+# # Add more tests based on the scenarios and edge cases you want to cover.
diff --git a/train.py b/train.py
deleted file mode 100644
index ee6f910..0000000
--- a/train.py
+++ /dev/null
@@ -1,112 +0,0 @@
-import gzip
-import random
-
-import numpy as np
-import torch
-import tqdm
-from torch.utils.data import DataLoader, Dataset
-from zeta.optim import StableAdamWUnfused
-
-from bitnet.at import AutoregressiveWrapper
-from bitnet import BitNetTransformer
-
-# constants
-
-NUM_BATCHES = int(1e5)
-BATCH_SIZE = 4
-GRADIENT_ACCUMULATE_EVERY = 4
-LEARNING_RATE = 2e-4
-VALIDATE_EVERY = 100
-GENERATE_EVERY = 500
-GENERATE_LENGTH = 512
-SEQ_LEN = 1024
-
-
-# helpers
-def cycle(loader):
-    while True:
-        yield from loader
-
-
-def decode_token(token):
-    return str(chr(max(32, token)))
-
-
-def decode_tokens(tokens):
-    return "".join(list(map(decode_token, tokens)))
-
-
-# instantiate GPT-like decoder model
-model = BitNetTransformer(num_tokens=256, dim=512, depth=8)
-model = AutoregressiveWrapper(model, max_seq_len=SEQ_LEN)
-
-
-# Use all available GPUs
-if torch.cuda.device_count() > 1:
-    print("Using", torch.cuda.device_count(), "GPUs!")
-    model = torch.nn.DataParallel(model)
-
-
-model.cuda()
-
-# prepare enwik8 data
-with gzip.open("./data/enwik8.gz") as file:
-    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)
-    trX, vaX = np.split(X, [int(90e6)])
-    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)
-
-
-class TextSamplerDataset(Dataset):
-    def __init__(self, data, seq_len):
-        super().__init__()
-        self.data = data
-        self.seq_len = seq_len
-
-    def __getitem__(self, index):
-        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))
-        full_seq = self.data[rand_start : rand_start + self.seq_len + 1].long()
-        return full_seq.cuda()
-
-    def __len__(self):
-        return self.data.size(0) // self.seq_len
-
-
-train_dataset = TextSamplerDataset(data_train, SEQ_LEN)
-val_dataset = TextSamplerDataset(data_val, SEQ_LEN)
-train_loader = cycle(DataLoader(train_dataset, batch_size=BATCH_SIZE))
-val_loader = cycle(DataLoader(val_dataset, batch_size=BATCH_SIZE))
-
-# optimizer
-optim = StableAdamWUnfused(
-    model.parameters(),
-    lr=LEARNING_RATE,
-)
-
-# training
-for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10.0, desc="training"):
-    model.train()
-
-    for __ in range(GRADIENT_ACCUMULATE_EVERY):
-        loss = model(next(train_loader))
-        loss.mean().backward()
-
-    print(f"training loss: {loss.mean().item()}")
-    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
-    optim.step()
-    optim.zero_grad()
-
-    if i % VALIDATE_EVERY == 0:
-        model.eval()
-        with torch.no_grad():
-            loss = model(next(val_loader))
-            print(f"validation loss: {loss.mean().item()}")
-
-    if i % GENERATE_EVERY == 0:
-        model.eval()
-        inp = random.choice(val_dataset)[:-1]
-        prime = decode_tokens(inp)
-        print("%s \n\n %s", (prime, "*" * 100))
-
-        sample = model.module.generate(inp[None, ...], GENERATE_LENGTH)
-        output_str = decode_tokens(sample[0])
-        print(output_str)
diff --git a/transformer_example.py b/transformer_example.py
deleted file mode 100644
index aed8a7b..0000000
--- a/transformer_example.py
+++ /dev/null
@@ -1,21 +0,0 @@
-# Import the necessary libraries
-import torch
-from bitnet import BitNetTransformer
-
-# Create a random tensor of integers
-x = torch.randint(0, 20000, (1, 1024))
-
-# Initialize the BitNetTransformer model
-bitnet = BitNetTransformer(
-    num_tokens=20000,  # Number of unique tokens in the input
-    dim=1024,  # Dimension of the input and output embeddings
-    depth=6,  # Number of transformer layers
-    heads=8,  # Number of attention heads
-    ff_mult=4,  # Multiplier for the hidden dimension in the feed-forward network
-)
-
-# Pass the tensor through the transformer model
-logits = bitnet(x)
-
-# Print the shape of the output
-print(logits)
